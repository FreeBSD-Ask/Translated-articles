# FreeBSD ä¼ä¸šçº§ 1 PB å­˜å‚¨

- [FreeBSD Enterprise 1 PB Storage](https://vermaden.wordpress.com/2019/06/19/freebsd-enterprise-1-pb-storage/)
- ä½œè€…ï¼šğšŸğšğš›ğš–ğšŠğšğšğš—
- 2019/06

ä»Šå¤© FreeBSD æ“ä½œç³»ç»Ÿè¿æ¥äº† 26 å‘¨å²ç”Ÿæ—¥ã€‚6 æœˆ 19 æ—¥æ˜¯ **å›½é™… FreeBSD æ—¥**ã€‚æ‰€ä»¥ä»Šå¤©æˆ‘å‡†å¤‡äº†ä¸€äº›ç‰¹åˆ«çš„å†…å®¹ ğŸ™‚ã€‚å¦‚ä½•åœ¨çœŸå®ç¡¬ä»¶ä¸Šä½¿ç”¨ FreeBSD æ„å»ºä¼ä¸šçº§å­˜å‚¨è§£å†³æ–¹æ¡ˆï¼Ÿè¿™æ­£æ˜¯ FreeBSD å‘æŒ¥å…¶æ‰€æœ‰å­˜å‚¨ç‰¹æ€§ï¼ˆåŒ…æ‹¬ ZFSï¼‰ä¼˜åŠ¿çš„åœ°æ–¹ã€‚

ä»Šå¤©æˆ‘å°†å±•ç¤ºæˆ‘å¦‚ä½•åŸºäº FreeBSD ç³»ç»Ÿæ„å»ºæ‰€è°“çš„ä¼ä¸šçº§å­˜å‚¨ï¼ŒåŒæ—¶æä¾›é€¾ 1 PBï¼ˆæ‹å­—èŠ‚ï¼‰çš„åŸå§‹å­˜å‚¨å®¹é‡ã€‚

æˆ‘æ›¾åŸºäº FreeBSD æ„å»ºè¿‡å„ç§å­˜å‚¨ç›¸å…³ç³»ç»Ÿï¼š

* [FreeBSD ä¸ŠåŸºäº Minio çš„åˆ†å¸ƒå¼å¯¹è±¡å­˜å‚¨](https://vermaden.wordpress.com/2018/04/16/distributed-object-storage-with-minio-on-freebsd/)
* [FreeBSD ä¸Šçš„ GlusterFS é›†ç¾¤ï¼Œç»“åˆ Ansible å’Œ GNU Parallel](https://vermaden.wordpress.com/2019/01/07/glusterfs-cluster-on-freebsd-with-ansible-and-gnu-parallel/)
* [é™éŸ³æ— é£æ‰‡ FreeBSD æœåŠ¡å™¨ â€”â€” å†—ä½™å¤‡ä»½](https://vermaden.wordpress.com/2019/04/03/silent-fanless-freebsd-server-redundant-backup/)

è¿™ä¸ªé¡¹ç›®æœ‰æ‰€ä¸åŒã€‚ä¸€å° 4U æœåŠ¡å™¨æœ€å¤šèƒ½æä¾›å¤šå°‘å­˜å‚¨ç©ºé—´ï¼Ÿäº‹å®è¯æ˜ï¼Œéå¸¸å¤šï¼ç»å¯¹å¤§äº 1 PBï¼ˆ1024 TBï¼‰çš„åŸå§‹å­˜å‚¨ç©ºé—´ã€‚

## ç¡¬ä»¶

è¿™äº›æ˜¯ 4U æœåŠ¡å™¨ï¼Œå¸¦æœ‰ 90-100 ä¸ª 3.5 å¯¸ç¡¬ç›˜ä½ï¼Œå¯ä»¥å®‰è£… 1260-1400 TB æ•°æ®ï¼ˆä½¿ç”¨ 14 TB ç¡¬ç›˜ï¼‰ã€‚æ­¤ç±»ç³»ç»Ÿç¤ºä¾‹æœ‰ï¼š

* [TYAN Thunder SX FA100](https://www.tyan.com/Barebones_FA100B7118_B7118F100V100HR)ï¼ˆ100 ä¸ªç¡¬ç›˜ä½ï¼‰
* [Supermicro SuperStorage 6048R-E1CR90L](https://www.supermicro.com/products/system/4U/6048/SSG-6048R-E1CR90L.cfm)ï¼ˆ90 ä¸ªç¡¬ç›˜ä½ï¼‰

æˆ‘ä¼šé€‰æ‹©ç¬¬ä¸€æ¬¾ â€”â€” ç®€ç§° TYAN FA100ã€‚

![logo-tyan.png](https://vermaden.wordpress.com/wp-content/uploads/2019/06/logo-tyan.png?w=960)

è™½ç„¶ä¹‹å‰çš„ *GlusterFS* å’Œ *Minio* é›†ç¾¤æ˜¯åœ¨è™šæ‹Ÿç¡¬ä»¶ï¼ˆç”šè‡³ FreeBSD Jail å®¹å™¨ï¼‰ä¸Šæ­å»ºçš„ï¼Œä½†è¿™ä¸ªé¡¹ç›®ä½¿ç”¨äº†çœŸå®ç‰©ç†ç¡¬ä»¶ã€‚

è¯¥ç³»ç»Ÿçš„è§„æ ¼å¦‚ä¸‹ï¼š

```
2 x 10 æ ¸ Intel Xeon Silver 4114 CPU @ 2.20GHz
4 x 32 GB DDR4 å†…å­˜ï¼ˆæ€»è®¡ 128 GBï¼‰
2 x Intel SSD DC S3500 240 GBï¼ˆç³»ç»Ÿç›˜ï¼‰
90 x Toshiba HDD MN07ACA12TE 12 TBï¼ˆæ•°æ®ç›˜ï¼‰
2 x Broadcom SAS3008 æ§åˆ¶å™¨
2 x Intel X710 DA-2 10GE ç½‘å¡
2 x ç”µæº
```

æ•´å¥—ç³»ç»Ÿä»·æ ¼çº¦ä¸º **$65,000**ï¼ˆå«ç¡¬ç›˜ï¼‰ã€‚å¤–è§‚å¦‚ä¸‹ï¼š

![tyan-fa100-small.jpg](https://vermaden.wordpress.com/wp-content/uploads/2019/06/tyan-fa100-small.jpg?w=960)

ä½ éœ€è¦é•¿è¾¾ 1200 mm çš„æœºæ¶æœºæŸœæ¥æ”¾ç½®è¿™å°å·¨å…½ :ğŸ™‚:

## ç®¡ç†ç•Œé¢

æ‰€è°“çš„ *Lights Out* ç®¡ç†ç•Œé¢éå¸¸ä¼˜ç§€ã€‚ç•Œé¢ç®€æ´ã€ç»„ç»‡è‰¯å¥½ã€å“åº”è¿…é€Ÿã€‚å¯ä»¥åˆ›å»ºå¤šä¸ªç‹¬ç«‹ç”¨æˆ·è´¦æˆ·ï¼Œä¹Ÿå¯ä»¥è¿æ¥å¤–éƒ¨ç”¨æˆ·æœåŠ¡ï¼Œå¦‚ LDAP/AD/RADIUSã€‚

![n01.png](https://vermaden.wordpress.com/wp-content/uploads/2019/06/n01.png?w=960)

ç™»å½•åï¼Œç®€æ´çš„ *Dashboard* åœ¨æ¬¢è¿æˆ‘ä»¬ã€‚

![n02.png](https://vermaden.wordpress.com/wp-content/uploads/2019/06/n02.png?w=960)

å¯ä»¥è·å–å„ç§ *ä¼ æ„Ÿå™¨* ä¿¡æ¯ï¼ŒåŒ…æ‹¬ç³»ç»Ÿç»„ä»¶æ¸©åº¦ã€‚

![n03](https://vermaden.wordpress.com/wp-content/uploads/2019/06/n03.png?w=960)

è¿˜å¯ä»¥æŸ¥çœ‹ *ç³»ç»Ÿåº“å­˜* ä¿¡æ¯ï¼Œäº†è§£å·²å®‰è£…ç¡¬ä»¶ã€‚

![n04.png](https://vermaden.wordpress.com/wp-content/uploads/2019/06/n04.png?w=960)

æœ‰ç‹¬ç«‹çš„ *è®¾ç½®* èœå•ï¼Œç”¨äºå„ç§é…ç½®é€‰é¡¹ã€‚

![n05.png](https://vermaden.wordpress.com/wp-content/uploads/2019/06/n05.png?w=960)

è™½ç„¶æ˜¯ 2019 å¹´ï¼Œä½†åªéœ€ HTML5 çš„ *è¿œç¨‹æ§åˆ¶*ï¼ˆè¿œç¨‹æ§åˆ¶å°ï¼‰ï¼Œæ— éœ€ä»»ä½•ç¬¬ä¸‰æ–¹æ’ä»¶å¦‚ Java/Silverlight/Flash ç­‰ï¼Œéå¸¸æ–¹ä¾¿ï¼Œä¹Ÿè¿è¡Œè‰¯å¥½ã€‚

![n06.png](https://vermaden.wordpress.com/wp-content/uploads/2019/06/n06.png?w=960)

![n07.png](https://vermaden.wordpress.com/wp-content/uploads/2019/06/n07.png?w=960)

å½“ç„¶å¯ä»¥è¿œç¨‹å¼€å…³æœºæˆ–å¾ªç¯é‡å¯ä¸»æœºã€‚

![n08.png](https://vermaden.wordpress.com/wp-content/uploads/2019/06/n08.png?w=960)

*ç»´æŠ¤* èœå•ç”¨äº BIOS æ›´æ–°ã€‚

![n09.png](https://vermaden.wordpress.com/wp-content/uploads/2019/06/n09.png?w=960)

## BIOS/UEFI

è¿›å…¥ BIOS/UEFI åï¼Œå¯ä»¥é€‰æ‹©ä»å“ªäº›ç¡¬ç›˜å¯åŠ¨ã€‚æˆªå›¾ä¸­æ˜¾ç¤ºä¸ºä¸¤å—å›ºæ€ç³»ç»Ÿç›˜ã€‚

![nas01.png](https://vermaden.wordpress.com/wp-content/uploads/2019/06/nas01.png?w=960)

BIOS/UEFI ç•Œé¢æ˜¾ç¤ºä¸¤ä¸ª *Enclosures*ï¼Œå®é™…ä¸Šæ˜¯ä¸¤å— Broadcom SAS3008 æ§åˆ¶å™¨ã€‚ä¸€äº›ç¡¬ç›˜è¿æ¥åˆ°ç¬¬ä¸€ä¸ª SAS æ§åˆ¶å™¨ï¼Œå…¶ä½™è¿æ¥åˆ°ç¬¬äºŒä¸ªï¼Œä»–ä»¬ç§°ä¹‹ä¸º *Enclosures* è€Œéæ§åˆ¶å™¨ï¼ŒåŸå› ä¸æ˜ã€‚

![nas05.png](https://vermaden.wordpress.com/wp-content/uploads/2019/06/nas05.png?w=960)

## FreeBSD ç³»ç»Ÿ

æˆ‘é€‰æ‹©äº†æœ€æ–°çš„ FreeBSD 12.0-RELEASE æ¥è¿›è¡Œå®‰è£…ã€‚å®‰è£…è¿‡ç¨‹å¤šâ€œé»˜è®¤â€ï¼Œç³»ç»Ÿç›˜ä½¿ç”¨ä¸¤å— SSD åš ZFS é•œåƒï¼Œæ²¡æœ‰ç‰¹åˆ«é…ç½®ã€‚

![logo-freebsd.jpg](https://vermaden.wordpress.com/wp-content/uploads/2019/06/logo-freebsd.jpg?w=960)

è¯¥å®‰è£…å½“ç„¶æ”¯æŒ [ZFS Boot Environments](https://vermaden.wordpress.com/2018/11/15/zfs-boot-environments-reloaded-at-nluug-autumn-conference-2018/) åŠŸèƒ½ï¼Œå¯å®ç°å®‰å…¨å‡çº§å’Œç³»ç»Ÿå˜æ›´ã€‚

```sh
# zpool list zroot
NAME    SIZE  ALLOC   FREE  CKPOINT  EXPANDSZ   FRAG    CAP  DEDUP  HEALTH  ALTROOT
zroot   220G  3.75G   216G        -         -     0%     1%  1.00x  ONLINE  -

# zpool status zroot
  pool: zroot
 state: ONLINE
  scan: none requested
config:

        NAME        STATE     READ WRITE CKSUM
        zroot       ONLINE       0     0     0
          mirror-0  ONLINE       0     0     0
            da91p4  ONLINE       0     0     0
            da11p4  ONLINE       0     0     0

errors: No known data errors

# df -g
Filesystem              1G-blocks Used  Avail Capacity  Mounted on
zroot/ROOT/default            211    2    209     1%    /
devfs                           0    0      0   100%    /dev
zroot/tmp                     209    0    209     0%    /tmp
zroot/usr/home                209    0    209     0%    /usr/home
zroot/usr/ports               210    0    209     0%    /usr/ports
zroot/usr/src                 210    0    209     0%    /usr/src
zroot/var/audit               209    0    209     0%    /var/audit
zroot/var/crash               209    0    209     0%    /var/crash
zroot/var/log                 209    0    209     0%    /var/log
zroot/var/mail                209    0    209     0%    /var/mail
zroot/var/tmp                 209    0    209     0%    /var/tmp

# beadm list
BE      Active Mountpoint  Space Created
default NR     /            2.4G 2019-05-24 13:24
```

## ç¡¬ç›˜å‡†å¤‡

åœ¨æ‰€æœ‰ 90 å— 12 TB ç¡¬ç›˜çš„å¯èƒ½æ–¹æ¡ˆä¸­ï¼Œæˆ‘é€‰æ‹©ä½¿ç”¨ RAID60 â€”â€” å½“ç„¶è¿™æ˜¯ ZFS çš„ç­‰æ•ˆæ–¹æ¡ˆã€‚æ¯ä¸ª RAID6ï¼ˆraidz2ï¼‰ç»„ä½¿ç”¨ 12 å—ç¡¬ç›˜ï¼Œæ€»å…±ä¼šæœ‰ 7 ä¸ªè¿™æ ·çš„ç»„ â€”â€” è¿™æ · ZFS æ± å°†ä½¿ç”¨ 84 å—ç¡¬ç›˜ï¼Œå‰©ä¸‹ 6 å—ä½œä¸ºå¤‡ç”¨ï¼ˆSPAREï¼‰ç¡¬ç›˜ â€”â€” å¯¹æˆ‘æ¥è¯´éå¸¸åˆé€‚ã€‚ç¡¬ç›˜åˆ†å¸ƒå¤§è‡´å¦‚ä¸‹ã€‚

```sh
DISKS  CONTENT
   12  raidz2-0
   12  raidz2-1
   12  raidz2-2
   12  raidz2-3
   12  raidz2-4
   12  raidz2-5
   12  raidz2-6
    6  spares
   90  TOTAL
```

ä¸‹é¢æ˜¯ FreeBSD ç³»ç»Ÿé€šè¿‡å‘½ä»¤ **camcontrol(8)** çœ‹åˆ°çš„è¿™äº›ç¡¬ç›˜æƒ…å†µã€‚æŒ‰è¿æ¥çš„ SAS æ§åˆ¶å™¨ â€”â€” **scbus(4)** æ’åºã€‚


```sh
# camcontrol devlist | sort -k 6
(AHCI SGPIO Enclosure 1.00 0001)   at scbus2 target 0 lun 0 (pass0,ses0)
(ATA TOSHIBA MG07ACA1 0101)        at scbus3 target 50 lun 0 (pass1,da0)
(ATA TOSHIBA MG07ACA1 0101)        at scbus3 target 52 lun 0 (pass2,da1)
(ATA TOSHIBA MG07ACA1 0101)        at scbus3 target 54 lun 0 (pass3,da2)
(ATA TOSHIBA MG07ACA1 0101)        at scbus3 target 56 lun 0 (pass5,da4)
(ATA TOSHIBA MG07ACA1 0101)        at scbus3 target 57 lun 0 (pass6,da5)
(ATA TOSHIBA MG07ACA1 0101)        at scbus3 target 59 lun 0 (pass7,da6)
(ATA TOSHIBA MG07ACA1 0101)        at scbus3 target 60 lun 0 (pass8,da7)
(ATA TOSHIBA MG07ACA1 0101)        at scbus3 target 66 lun 0 (pass9,da8)
(ATA TOSHIBA MG07ACA1 0101)        at scbus3 target 67 lun 0 (pass10,da9)
(ATA TOSHIBA MG07ACA1 0101)        at scbus3 target 74 lun 0 (pass11,da10)
(ATA INTEL SSDSC2KB24 0100)        at scbus3 target 75 lun 0 (pass12,da11)
(ATA TOSHIBA MG07ACA1 0101)        at scbus3 target 76 lun 0 (pass13,da12)
(ATA TOSHIBA MG07ACA1 0101)        at scbus3 target 82 lun 0 (pass14,da13)
(ATA TOSHIBA MG07ACA1 0101)        at scbus3 target 83 lun 0 (pass15,da14)
(ATA TOSHIBA MG07ACA1 0101)        at scbus3 target 85 lun 0 (pass16,da15)
(ATA TOSHIBA MG07ACA1 0101)        at scbus3 target 87 lun 0 (pass17,da16)
(Tyan B7118 0500)                  at scbus3 target 88 lun 0 (pass18,ses1)
(ATA TOSHIBA MG07ACA1 0101)        at scbus3 target 89 lun 0 (pass19,da17)
(ATA TOSHIBA MG07ACA1 0101)        at scbus3 target 90 lun 0 (pass20,da18)
(ATA TOSHIBA MG07ACA1 0101)        at scbus3 target 91 lun 0 (pass21,da19)
(ATA TOSHIBA MG07ACA1 0101)        at scbus3 target 92 lun 0 (pass22,da20)
(ATA TOSHIBA MG07ACA1 0101)        at scbus3 target 93 lun 0 (pass23,da21)
(ATA TOSHIBA MG07ACA1 0101)        at scbus3 target 94 lun 0 (pass24,da22)
(ATA TOSHIBA MG07ACA1 0101)        at scbus3 target 95 lun 0 (pass25,da23)
(ATA TOSHIBA MG07ACA1 0101)        at scbus3 target 96 lun 0 (pass26,da24)
(ATA TOSHIBA MG07ACA1 0101)        at scbus3 target 97 lun 0 (pass27,da25)
(ATA TOSHIBA MG07ACA1 0101)        at scbus3 target 98 lun 0 (pass28,da26)
(ATA TOSHIBA MG07ACA1 0101)        at scbus3 target 99 lun 0 (pass29,da27)
(ATA TOSHIBA MG07ACA1 0101)        at scbus3 target 100 lun 0 (pass30,da28)
(ATA TOSHIBA MG07ACA1 0101)        at scbus3 target 101 lun 0 (pass31,da29)
(ATA TOSHIBA MG07ACA1 0101)        at scbus3 target 102 lun 0 (pass32,da30)
(ATA TOSHIBA MG07ACA1 0101)        at scbus3 target 103 lun 0 (pass33,da31)
(ATA TOSHIBA MG07ACA1 0101)        at scbus3 target 104 lun 0 (pass34,da32)
(ATA TOSHIBA MG07ACA1 0101)        at scbus3 target 105 lun 0 (pass35,da33)
(ATA TOSHIBA MG07ACA1 0101)        at scbus3 target 106 lun 0 (pass36,da34)
(ATA TOSHIBA MG07ACA1 0101)        at scbus3 target 107 lun 0 (pass37,da35)
(ATA TOSHIBA MG07ACA1 0101)        at scbus3 target 108 lun 0 (pass38,da36)
(ATA TOSHIBA MG07ACA1 0101)        at scbus3 target 109 lun 0 (pass39,da37)
(ATA TOSHIBA MG07ACA1 0101)        at scbus3 target 110 lun 0 (pass40,da38)
(ATA TOSHIBA MG07ACA1 0101)        at scbus4 target 48 lun 0 (pass41,da39)
(ATA TOSHIBA MG07ACA1 0101)        at scbus4 target 49 lun 0 (pass42,da40)
(ATA TOSHIBA MG07ACA1 0101)        at scbus4 target 51 lun 0 (pass43,da41)
(ATA TOSHIBA MG07ACA1 0101)        at scbus4 target 53 lun 0 (pass44,da42)
(ATA TOSHIBA MG07ACA1 0101)        at scbus4 target 55 lun 0 (da43,pass45)
(ATA TOSHIBA MG07ACA1 0101)        at scbus4 target 59 lun 0 (pass46,da44)
(ATA TOSHIBA MG07ACA1 0101)        at scbus4 target 64 lun 0 (pass47,da45)
(ATA TOSHIBA MG07ACA1 0101)        at scbus4 target 67 lun 0 (pass48,da46)
(ATA TOSHIBA MG07ACA1 0101)        at scbus4 target 68 lun 0 (pass49,da47)
(ATA TOSHIBA MG07ACA1 0101)        at scbus4 target 69 lun 0 (pass50,da48)
(ATA TOSHIBA MG07ACA1 0101)        at scbus4 target 73 lun 0 (pass51,da49)
(ATA TOSHIBA MG07ACA1 0101)        at scbus4 target 76 lun 0 (pass52,da50)
(ATA TOSHIBA MG07ACA1 0101)        at scbus4 target 77 lun 0 (pass53,da51)
(Tyan B7118 0500)                  at scbus4 target 80 lun 0 (pass54,ses2)
(ATA TOSHIBA MG07ACA1 0101)        at scbus4 target 81 lun 0 (pass55,da52)
(ATA TOSHIBA MG07ACA1 0101)        at scbus4 target 82 lun 0 (pass56,da53)
(ATA TOSHIBA MG07ACA1 0101)        at scbus4 target 83 lun 0 (pass57,da54)
(ATA TOSHIBA MG07ACA1 0101)        at scbus4 target 84 lun 0 (pass58,da55)
(ATA TOSHIBA MG07ACA1 0101)        at scbus4 target 85 lun 0 (pass59,da56)
(ATA TOSHIBA MG07ACA1 0101)        at scbus4 target 86 lun 0 (pass60,da57)
(ATA TOSHIBA MG07ACA1 0101)        at scbus4 target 87 lun 0 (pass61,da58)
(ATA TOSHIBA MG07ACA1 0101)        at scbus4 target 88 lun 0 (pass62,da59)
(ATA TOSHIBA MG07ACA1 0101)        at scbus4 target 89 lun 0 (da63,pass66)
(ATA TOSHIBA MG07ACA1 0101)        at scbus4 target 90 lun 0 (pass64,da61)
(ATA TOSHIBA MG07ACA1 0101)        at scbus4 target 91 lun 0 (pass65,da62)
(ATA TOSHIBA MG07ACA1 0101)        at scbus4 target 92 lun 0 (da60,pass63)
(ATA TOSHIBA MG07ACA1 0101)        at scbus4 target 94 lun 0 (pass67,da64)
(ATA TOSHIBA MG07ACA1 0101)        at scbus4 target 97 lun 0 (pass68,da65)
(ATA TOSHIBA MG07ACA1 0101)        at scbus4 target 98 lun 0 (pass69,da66)
(ATA TOSHIBA MG07ACA1 0101)        at scbus4 target 99 lun 0 (pass70,da67)
(ATA TOSHIBA MG07ACA1 0101)        at scbus4 target 100 lun 0 (pass71,da68)
(Tyan B7118 0500)                  at scbus4 target 101 lun 0 (pass72,ses3)
(ATA TOSHIBA MG07ACA1 0101)        at scbus4 target 102 lun 0 (pass73,da69)
(ATA TOSHIBA MG07ACA1 0101)        at scbus4 target 103 lun 0 (pass74,da70)
(ATA TOSHIBA MG07ACA1 0101)        at scbus4 target 104 lun 0 (pass75,da71)
(ATA TOSHIBA MG07ACA1 0101)        at scbus4 target 105 lun 0 (pass76,da72)
(ATA TOSHIBA MG07ACA1 0101)        at scbus4 target 106 lun 0 (pass77,da73)
(ATA TOSHIBA MG07ACA1 0101)        at scbus4 target 107 lun 0 (pass78,da74)
(ATA TOSHIBA MG07ACA1 0101)        at scbus4 target 108 lun 0 (pass79,da75)
(ATA TOSHIBA MG07ACA1 0101)        at scbus4 target 109 lun 0 (pass80,da76)
(ATA TOSHIBA MG07ACA1 0101)        at scbus4 target 110 lun 0 (pass81,da77)
(ATA TOSHIBA MG07ACA1 0101)        at scbus4 target 111 lun 0 (pass82,da78)
(ATA TOSHIBA MG07ACA1 0101)        at scbus4 target 112 lun 0 (pass83,da79)
(ATA TOSHIBA MG07ACA1 0101)        at scbus4 target 113 lun 0 (pass84,da80)
(ATA TOSHIBA MG07ACA1 0101)        at scbus4 target 114 lun 0 (pass85,da81)
(ATA TOSHIBA MG07ACA1 0101)        at scbus4 target 115 lun 0 (pass86,da82)
(ATA TOSHIBA MG07ACA1 0101)        at scbus4 target 116 lun 0 (pass87,da83)
(ATA TOSHIBA MG07ACA1 0101)        at scbus4 target 117 lun 0 (pass88,da84)
(ATA TOSHIBA MG07ACA1 0101)        at scbus4 target 118 lun 0 (pass89,da85)
(ATA TOSHIBA MG07ACA1 0101)        at scbus4 target 119 lun 0 (pass90,da86)
(ATA TOSHIBA MG07ACA1 0101)        at scbus4 target 120 lun 0 (pass91,da87)
(ATA TOSHIBA MG07ACA1 0101)        at scbus4 target 121 lun 0 (pass92,da88)
(ATA TOSHIBA MG07ACA1 0101)        at scbus4 target 122 lun 0 (pass93,da89)
(ATA TOSHIBA MG07ACA1 0101)        at scbus4 target 123 lun 0 (pass94,da90)
(ATA INTEL SSDSC2KB24 0100)        at scbus4 target 124 lun 0 (pass95,da91)
(ATA TOSHIBA MG07ACA1 0101)        at scbus4 target 125 lun 0 (da3,pass4)
```

æœ‰äººå¯èƒ½ä¼šé—®ï¼Œå½“ç¡¬ç›˜å‡ºç°æ•…éšœæ—¶å¦‚ä½•è¯†åˆ«æ˜¯å“ªä¸€å—â€¦â€¦è¿™æ—¶ FreeBSD çš„ **<sesutil(8)>** å‘½ä»¤å°±éå¸¸æœ‰ç”¨äº†ã€‚

```sh
# sesutil locate all off
# sesutil locate da64 on
```

ç¬¬ä¸€æ¡ **sesutil(8)** å‘½ä»¤å…³é—­æœºç®±ä¸­æ‰€æœ‰ç¡¬ç›˜çš„ä½ç½®æŒ‡ç¤ºç¯ã€‚ç¬¬äºŒæ¡å‘½ä»¤åˆ™ç‚¹äº®äº† **da64** ç¡¬ç›˜çš„è¯†åˆ«ç¯ã€‚

æˆ‘è¿˜ä¼šç¡®ä¿ä¸ä¼šä½¿ç”¨æ¯å—ç¡¬ç›˜çš„å…¨éƒ¨å®¹é‡ã€‚è¿™ä¸ªæƒ³æ³•çœ‹ä¼¼æ— æ„ä¹‰ï¼Œä½†è®¾æƒ³ä»¥ä¸‹æƒ…å†µï¼šäº”å— 12 TB ç¡¬ç›˜åœ¨ä¸‰å¹´ååŒæ—¶æŸåï¼Œä½ æ— æ³•è·å¾—ç›¸åŒå‹å·çš„ç¡¬ç›˜ï¼Œåªèƒ½ç”¨å…¶ä»– 12 TB ç¡¬ç›˜æ›¿ä»£ï¼Œç”šè‡³å¯èƒ½æ¥è‡ªä¸åŒå‚å•†ã€‚

```sh
# grep da64 /var/run/dmesg.boot
da64 at mpr1 bus 0 scbus4 target 93 lun 0
da64:  Fixed Direct Access SPC-4 SCSI device
da64: Serial Number 98G0A1EQF95G
da64: 1200.000MB/s transfers
da64: Command Queueing enabled
da64: 11444224MB (23437770752 512 byte sectors)
```

å•å— 12 TB ç¡¬ç›˜å…±æœ‰ **23437770752** ä¸ª **512** å­—èŠ‚çš„æ‰‡åŒºï¼Œæ€»åŸå§‹å®¹é‡ä¸º **12000138625024** å­—èŠ‚ã€‚

```sh
# expr 23437770752 \* 512
12000138625024
```

ç°åœ¨å‡è®¾è¿™äº›æ¥è‡ªå…¶ä»–å‚å•†çš„ 12 TB ç¡¬ç›˜æ¯å—æ¯”åŸæ¥çš„å°‘ 4 å­—èŠ‚ â€”â€” ZFS å°†ä¸å…è®¸ä½¿ç”¨å®ƒä»¬ï¼Œå› ä¸ºå®¹é‡ä¸è¶³ã€‚

å› æ­¤æˆ‘ä¼šå°†æ¯å—ç¡¬ç›˜çš„å®¹é‡è®¾ç½®ä¸º **11175 GB**ï¼Œå¤§çº¦æ¯”å…¶æ€»å®¹é‡ **11176 GB** å°‘ 1 GBã€‚

ä¸‹é¢æ˜¯å¯ä»¥å¯¹æ‰€æœ‰ 90 å—ç¡¬ç›˜æ‰§è¡Œæ­¤æ“ä½œçš„å‘½ä»¤ã€‚

```sh
# camcontrol devlist \
    | grep TOSHIBA \
    | awk '{print $NF}' \
    | awk -F ',' '{print $2}' \
    | tr -d ')' \
    | while read DISK
      do
        gpart destroy -F                   ${DISK} 1> /dev/null 2> /dev/null
        gpart create -s GPT                ${DISK}
        gpart add -t freebsd-zfs -s 11175G ${DISK}
      done

# gpart show da64
=>         40  23437770672  da64  GPT  (11T)
           40  23435673600     1  freebsd-zfs  (11T)
  23435673640      2097072        - free -  (1.0G)
```

## é…ç½® ZFS æ± 

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬éœ€è¦åˆ›å»º ZFS æ± ï¼Œè¿™å¯èƒ½æ˜¯æˆ‘æ‰§è¡Œè¿‡çš„æœ€é•¿çš„ **zpool** å‘½ä»¤äº† :ğŸ™‚:

ç”±äºä¸œèŠ 12 TB ç¡¬ç›˜ä½¿ç”¨ 4k æ‰‡åŒºï¼Œæˆ‘ä»¬éœ€è¦å°† **vfs.zfs.min_auto_ashift** è®¾ç½®ä¸º 12 æ¥å¼ºåˆ¶ä½¿ç”¨è¯¥æ‰‡åŒºå¤§å°ã€‚

```sh
# sysctl vfs.zfs.min_auto_ashift=12
vfs.zfs.min_auto_ashift: 12 -> 12

# zpool create nas02 \
    raidz2  da0p1  da1p1  da2p1  da3p1  da4p1  da5p1  da6p1  da7p1  da8p1  da9p1 da10p1 da12p1 \
    raidz2 da13p1 da14p1 da15p1 da16p1 da17p1 da18p1 da19p1 da20p1 da21p1 da22p1 da23p1 da24p1 \
    raidz2 da25p1 da26p1 da27p1 da28p1 da29p1 da30p1 da31p1 da32p1 da33p1 da34p1 da35p1 da36p1 \
    raidz2 da37p1 da38p1 da39p1 da40p1 da41p1 da42p1 da43p1 da44p1 da45p1 da46p1 da47p1 da48p1 \
    raidz2 da49p1 da50p1 da51p1 da52p1 da53p1 da54p1 da55p1 da56p1 da57p1 da58p1 da59p1 da60p1 \
    raidz2 da61p1 da62p1 da63p1 da64p1 da65p1 da66p1 da67p1 da68p1 da69p1 da70p1 da71p1 da72p1 \
    raidz2 da73p1 da74p1 da75p1 da76p1 da77p1 da78p1 da79p1 da80p1 da81p1 da82p1 da83p1 da84p1 \
    spare  da85p1 da86p1 da87p1 da88p1 da89p1 da90p1

# zpool status
  pool: nas02
 state: ONLINE
  scan: scrub repaired 0 in 0 days 00:00:05 with 0 errors on Fri May 31 10:26:29 2019
config:

        NAME        STATE     READ WRITE CKSUM
        nas02       ONLINE       0     0     0
          raidz2-0  ONLINE       0     0     0
            da0p1   ONLINE       0     0     0
            da1p1   ONLINE       0     0     0
            da2p1   ONLINE       0     0     0
            da3p1   ONLINE       0     0     0
            da4p1   ONLINE       0     0     0
            da5p1   ONLINE       0     0     0
            da6p1   ONLINE       0     0     0
            da7p1   ONLINE       0     0     0
            da8p1   ONLINE       0     0     0
            da9p1   ONLINE       0     0     0
            da10p1  ONLINE       0     0     0
            da12p1  ONLINE       0     0     0
          raidz2-1  ONLINE       0     0     0
            da13p1  ONLINE       0     0     0
            da14p1  ONLINE       0     0     0
            da15p1  ONLINE       0     0     0
            da16p1  ONLINE       0     0     0
            da17p1  ONLINE       0     0     0
            da18p1  ONLINE       0     0     0
            da19p1  ONLINE       0     0     0
            da20p1  ONLINE       0     0     0
            da21p1  ONLINE       0     0     0
            da22p1  ONLINE       0     0     0
            da23p1  ONLINE       0     0     0
            da24p1  ONLINE       0     0     0
          raidz2-2  ONLINE       0     0     0
            da25p1  ONLINE       0     0     0
            da26p1  ONLINE       0     0     0
            da27p1  ONLINE       0     0     0
            da28p1  ONLINE       0     0     0
            da29p1  ONLINE       0     0     0
            da30p1  ONLINE       0     0     0
            da31p1  ONLINE       0     0     0
            da32p1  ONLINE       0     0     0
            da33p1  ONLINE       0     0     0
            da34p1  ONLINE       0     0     0
            da35p1  ONLINE       0     0     0
            da36p1  ONLINE       0     0     0
          raidz2-3  ONLINE       0     0     0
            da37p1  ONLINE       0     0     0
            da38p1  ONLINE       0     0     0
            da39p1  ONLINE       0     0     0
            da40p1  ONLINE       0     0     0
            da41p1  ONLINE       0     0     0
            da42p1  ONLINE       0     0     0
            da43p1  ONLINE       0     0     0
            da44p1  ONLINE       0     0     0
            da45p1  ONLINE       0     0     0
            da46p1  ONLINE       0     0     0
            da47p1  ONLINE       0     0     0
            da48p1  ONLINE       0     0     0
          raidz2-4  ONLINE       0     0     0
            da49p1  ONLINE       0     0     0
            da50p1  ONLINE       0     0     0
            da51p1  ONLINE       0     0     0
            da52p1  ONLINE       0     0     0
            da53p1  ONLINE       0     0     0
            da54p1  ONLINE       0     0     0
            da55p1  ONLINE       0     0     0
            da56p1  ONLINE       0     0     0
            da57p1  ONLINE       0     0     0
            da58p1  ONLINE       0     0     0
            da59p1  ONLINE       0     0     0
            da60p1  ONLINE       0     0     0
          raidz2-5  ONLINE       0     0     0
            da61p1  ONLINE       0     0     0
            da62p1  ONLINE       0     0     0
            da63p1  ONLINE       0     0     0
            da64p1  ONLINE       0     0     0
            da65p1  ONLINE       0     0     0
            da66p1  ONLINE       0     0     0
            da67p1  ONLINE       0     0     0
            da68p1  ONLINE       0     0     0
            da69p1  ONLINE       0     0     0
            da70p1  ONLINE       0     0     0
            da71p1  ONLINE       0     0     0
            da72p1  ONLINE       0     0     0
          raidz2-6  ONLINE       0     0     0
            da73p1  ONLINE       0     0     0
            da74p1  ONLINE       0     0     0
            da75p1  ONLINE       0     0     0
            da76p1  ONLINE       0     0     0
            da77p1  ONLINE       0     0     0
            da78p1  ONLINE       0     0     0
            da79p1  ONLINE       0     0     0
            da80p1  ONLINE       0     0     0
            da81p1  ONLINE       0     0     0
            da82p1  ONLINE       0     0     0
            da83p1  ONLINE       0     0     0
            da84p1  ONLINE       0     0     0
        spares
          da85p1    AVAIL
          da86p1    AVAIL
          da87p1    AVAIL
          da88p1    AVAIL
          da89p1    AVAIL
          da90p1    AVAIL

errors: No known data errors

# zpool list nas02
NAME    SIZE  ALLOC   FREE  CKPOINT  EXPANDSZ   FRAG    CAP  DEDUP  HEALTH  ALTROOT
nas02   915T  1.42M   915T        -         -     0%     0%  1.00x  ONLINE  -

# zfs list nas02
NAME    USED  AVAIL  REFER  MOUNTPOINT
nas02    88K   675T   201K  none
```

## ZFS è®¾ç½®

ç”±äºè¯¥å­˜å‚¨çš„ä¸»è¦ç”¨é€”æ˜¯å­˜æ”¾æ–‡ä»¶ï¼Œæˆ‘å°†ä½¿ç”¨ **recordsize** çš„æœ€å¤§å€¼ä¹‹ä¸€ â€”â€” 1 MB â€”â€” ä»¥è·å¾—æ›´å¥½çš„å‹ç¼©æ¯”ã€‚

â€¦ä½†å®ƒä¹Ÿå°†ä½œä¸º iSCSI ç›®æ ‡ä½¿ç”¨ï¼Œåœ¨ iSCSI ä¸­æˆ‘ä»¬ä¼šä½¿ç”¨åŸç”Ÿçš„ 4k å—ï¼Œå› æ­¤ iSCSI è®¾ç½®ä¸º 4096 å­—èŠ‚ã€‚

```sh
# zfs set compression=lz4         nas02
# zfs set atime=off               nas02
# zfs set mountpoint=none         nas02
# zfs set recordsize=1m           nas02
# zfs set redundant_metadata=most nas02
# zfs create                      nas02/nfs
# zfs create                      nas02/smb
# zfs create                      nas02/iscsi
# zfs set recordsize=4k           nas02/iscsi
```

å¦å¤–è¯´æ˜ä¸€ä¸‹å‚æ•° **redundant_metadata** ï¼Œå› ä¸ºå®ƒä¸æ˜¯å¾ˆç›´è§‚ã€‚å¼•ç”¨ **zfs(8)** æ‰‹å†Œä¸­çš„è¯´æ˜ã€‚

```sh
# man zfs
(...)
redundant_metadata=all | most
  Controls what types of metadata are stored redundantly.  ZFS stores
  an extra copy of metadata, so that if a single block is corrupted,
  the amount of user data lost is limited.  This extra copy is in
  addition to any redundancy provided at the pool level (e.g. by
  mirroring or RAID-Z), and is in addition to an extra copy specified
  by the copies property (up to a total of 3 copies).  For example if
  the pool is mirrored, copies=2, and redundant_metadata=most, then ZFS
  stores 6 copies of most metadata, and 4 copies of data and some
  metadata.

  When set to all, ZFS stores an extra copy of all metadata.  If a
  single on-disk block is corrupt, at worst a single block of user data
  (which is recordsize bytes long can be lost.)

  When set to most, ZFS stores an extra copy of most types of metadata.
  This can improve performance of random writes, because less metadata
  must be written.  In practice, at worst about 100 blocks (of
  recordsize bytes each) of user data can be lost if a single on-disk
  block is corrupt.  The exact behavior of which metadata blocks are
  stored redundantly may change in future releases.

  The default value is all.

æ§åˆ¶å­˜å‚¨å“ªäº›ç±»å‹çš„å…ƒæ•°æ®ä¸ºå†—ä½™ã€‚
ZFS ä¼šå­˜å‚¨é¢å¤–çš„å…ƒæ•°æ®å‰¯æœ¬ï¼Œä»¥ä¾¿å½“å•ä¸ªå—æŸåæ—¶ï¼Œç”¨æˆ·æ•°æ®çš„ä¸¢å¤±é‡æœ‰é™ã€‚
è¿™ä¸ªé¢å¤–å‰¯æœ¬æ˜¯åœ¨æ± çº§åˆ«æä¾›çš„å†—ä½™ï¼ˆä¾‹å¦‚é•œåƒæˆ– RAID-Zï¼‰ä¹‹å¤–çš„ï¼Œå¹¶ä¸”æ˜¯åœ¨ **copies** å±æ€§æŒ‡å®šçš„é¢å¤–å‰¯æœ¬ä¹‹å¤–ï¼ˆæœ€å¤šå¯è¾¾ä¸‰ä»½ï¼‰ã€‚
ä¾‹å¦‚ï¼Œå¦‚æœæ± æ˜¯é•œåƒï¼Œcopies=2ï¼Œä¸” redundant_metadata=mostï¼Œé‚£ä¹ˆ ZFS ä¼šä¸ºå¤§éƒ¨åˆ†å…ƒæ•°æ®å­˜å‚¨ 6 ä»½å‰¯æœ¬ï¼Œä¸ºæ•°æ®å’Œéƒ¨åˆ†å…ƒæ•°æ®å­˜å‚¨ 4 ä»½å‰¯æœ¬ã€‚

å½“è®¾ç½®ä¸º **all** æ—¶ï¼ŒZFS ä¼šä¸ºæ‰€æœ‰å…ƒæ•°æ®å­˜å‚¨é¢å¤–å‰¯æœ¬ã€‚
å¦‚æœå•ä¸ªç£ç›˜å—æŸåï¼Œæœ€åæƒ…å†µä¸‹åªä¼šä¸¢å¤±ä¸€å—ç”¨æˆ·æ•°æ®ï¼ˆå¤§å°ä¸º recordsize å­—èŠ‚ï¼‰ã€‚

å½“è®¾ç½®ä¸º **most** æ—¶ï¼ŒZFS ä¼šä¸ºå¤§éƒ¨åˆ†ç±»å‹çš„å…ƒæ•°æ®å­˜å‚¨é¢å¤–å‰¯æœ¬ã€‚
è¿™å¯ä»¥æå‡éšæœºå†™å…¥æ€§èƒ½ï¼Œå› ä¸ºéœ€è¦å†™å…¥çš„å…ƒæ•°æ®æ›´å°‘ã€‚
å®é™…ä¸Šï¼Œå¦‚æœå•ä¸ªç£ç›˜å—æŸåï¼Œæœ€åæƒ…å†µä¸‹å¯èƒ½ä¸¢å¤±å¤§çº¦ 100 å—ç”¨æˆ·æ•°æ®ï¼ˆæ¯å—å¤§å°ä¸º recordsize å­—èŠ‚ï¼‰ã€‚
å…·ä½“å“ªäº›å…ƒæ•°æ®å—ä¼šè¢«å†—ä½™å­˜å‚¨ï¼Œæœªæ¥ç‰ˆæœ¬å¯èƒ½ä¼šæœ‰æ‰€å˜åŒ–ã€‚

é»˜è®¤å€¼ä¸º **all**ã€‚

(...)
```

ä»ä¸Šé¢çš„æ–‡å­—å¯ä»¥çœ‹å‡ºï¼Œå®ƒä¸»è¦åœ¨å•è®¾å¤‡æ± ä¸­æœ‰ç”¨ï¼Œå› ä¸ºå½“æˆ‘ä»¬åŸºäº RAIDZ2ï¼ˆRAID6 ç­‰ä»·ï¼‰æä¾›å†—ä½™æ—¶ï¼Œå°±ä¸éœ€è¦ä¿ç•™é¢å¤–çš„å…ƒæ•°æ®å‰¯æœ¬ã€‚

è¿™æ ·åšèƒ½æå‡å†™å…¥æ€§èƒ½ã€‚

ä¸ºäº†è®°å½•â€”â€”iSCSI çš„ ZFS zvol æ˜¯é€šè¿‡å¦‚ä¸‹å‘½ä»¤åˆ›å»ºçš„â€”â€”ä½œä¸ºç¨€ç–æ–‡ä»¶ï¼Œä¹Ÿç§°ä¸º *è–„é…ç½®ï¼ˆThin Provisioningï¼‰* æ¨¡å¼ã€‚

```sh
# zfs create -s -V 16T nas02/iscsi/test
```

ç”±äºæˆ‘ä»¬æœ‰å¤‡ç”¨ï¼ˆSPAREï¼‰ç£ç›˜ï¼Œæˆ‘ä»¬è¿˜éœ€è¦é€šè¿‡åœ¨ **/etc/rc.conf** æ–‡ä»¶ä¸­æ·»åŠ  **zfsd_enable=YES** æ¥å¯ç”¨ **zfsd(8)** å®ˆæŠ¤è¿›ç¨‹ã€‚

æˆ‘ä»¬è¿˜éœ€è¦ä¸ºæ± å¯ç”¨ **autoreplace** å±æ€§ï¼Œå› ä¸ºé»˜è®¤æƒ…å†µä¸‹å®ƒæ˜¯ **off**ã€‚

```sh
# zpool get autoreplace nas02
NAME   PROPERTY     VALUE    SOURCE
nas02  autoreplace  off      default

# zpool set autoreplace=on nas02

# zpool get autoreplace nas02
NAME   PROPERTY     VALUE    SOURCE
nas02  autoreplace  on       local
```

å…¶ä»– ZFS è®¾ç½®ä½äº **/boot/loader.conf** æ–‡ä»¶ä¸­ã€‚ç”±äºè¯¥ç³»ç»Ÿæœ‰ 128 GB å†…å­˜ï¼Œæˆ‘ä»¬å°†å…è®¸ ZFS ä½¿ç”¨å…¶ä¸­çš„ 50% åˆ° 75% ä½œä¸º ARCã€‚

```sh
# grep vfs.zfs /boot/loader.conf
  vfs.zfs.prefetch_disable=1
  vfs.zfs.cache_flush_disable=1
  vfs.zfs.vdev.cache.size=16M
  vfs.zfs.arc_min=64G
  vfs.zfs.arc_max=96G
  vfs.zfs.deadman_enabled=0
```

## é…ç½®ç½‘ç»œ

è¿™æ­£æ˜¯æˆ‘éå¸¸å–œæ¬¢ FreeBSD çš„åœ°æ–¹ã€‚è¦è®¾ç½® LACP é“¾è·¯èšåˆï¼Œä½ åªéœ€åœ¨ **/etc/rc.conf** æ–‡ä»¶ä¸­å†™ 5 è¡Œã€‚åœ¨ RHEL ä¸Šï¼Œä½ å¯èƒ½éœ€è¦å¤šä¸ªæ–‡ä»¶ï¼Œæ¯ä¸ªæ–‡ä»¶è¿˜è¦å†™å¾ˆå¤šè¡Œã€‚

```sh
# head -5 /etc/rc.conf
  defaultrouter="10.20.30.254"
  ifconfig_ixl0="up"
  ifconfig_ixl1="up"
  cloned_interfaces="lagg0"
  ifconfig_lagg0="laggproto lacp laggport ixl0 laggport ixl1 10.20.30.2/24 up"

# ifconfig lagg0
lagg0: flags=8843 metric 0 mtu 1500
        options=e507bb
        ether a0:42:3f:a0:42:3f
        inet 10.20.30.2 netmask 0xffffff00 broadcast 10.20.30.255
        laggproto lacp lagghash l2,l3,l4
        laggport: ixl0 flags=1c
        laggport: ixl1 flags=1c
        groups: lagg
        media: Ethernet autoselect
        status: active
        nd6 options=29
```

**Intel X710 DA-2** 10GE ç½‘å¡åœ¨ FreeBSD ä¸‹ç”± **ixl(4)** é©±åŠ¨å®Œç¾æ”¯æŒã€‚

![intel-x710-da-2.jpg](https://vermaden.wordpress.com/wp-content/uploads/2019/06/intel-x710-da-2.jpg?w=960)

### é…ç½® Cisco Nexus 

è¿™æ˜¯å¯ç”¨ LACP èšåˆæ‰€éœ€çš„ *Cisco Nexus* é…ç½®ã€‚

é¦–å…ˆæ˜¯ç«¯å£è®¾ç½®ã€‚

```sh
NEXUS-1  Eth1/32  NAS02_IXL0  connected 3  full  a-10G  SFP-H10GB-A
NEXUS-2  Eth1/32  NAS02_IXL1  connected 3  full  a-10G  SFP-H10GB-A
```

â€¦ ç°åœ¨è¿›è¡Œèšåˆé…ç½®ã€‚

```ini
interface Ethernet1/32
  description NAS02_IXL1
  switchport
  switchport access vlan 3
  mtu 9216
  channel-group 128 mode active
  no shutdown
!
interface port-channel128
  description NAS02
  switchport
  switchport access vlan 3
  mtu 9216
  vpc 128
```

â€¦ åœ¨ç¬¬äºŒå° *Cisco Nexus* **NEXUS-2** äº¤æ¢æœºä¸Šä¹Ÿè¿›è¡Œç›¸åŒ/ç±»ä¼¼é…ç½®ã€‚

## FreeBSD é…ç½®

è¿™äº›æ˜¯åœ¨ FreeBSD ç³»ç»Ÿä¸Šæœ€é‡è¦çš„ä¸‰ä¸ªé…ç½®æ–‡ä»¶ã€‚

ç°åœ¨æˆ‘å°†å‘å¸ƒæˆ‘åœ¨è¿™å°å­˜å‚¨ç³»ç»Ÿä¸Šä½¿ç”¨çš„æ‰€æœ‰è®¾ç½®ã€‚

**/etc/rc.conf** æ–‡ä»¶ã€‚

```ini
# cat /etc/rc.conf
# ç½‘ç»œ
  hostname="nas02.local"
  defaultrouter="10.20.30.254"
  ifconfig_ixl0="up"
  ifconfig_ixl1="up"
  cloned_interfaces="lagg0"
  ifconfig_lagg0="laggproto lacp laggport ixl0 laggport ixl1 10.20.30.2/24 up"

# å†…æ ¸æ¨¡å—
  kld_list="${kld_list} aesni"

# å®ˆæŠ¤è¿›ç¨‹ | å¯ç”¨
  zfs_enable=YES
  zfsd_enable=YES
  sshd_enable=YES
  ctld_enable=YES
  powerd_enable=YES

# å®ˆæŠ¤è¿›ç¨‹ | NFS æœåŠ¡
  nfs_server_enable=YES
  nfs_client_enable=YES
  rpc_lockd_enable=YES
  rpc_statd_enable=YES
  rpcbind_enable=YES
  mountd_enable=YES
  mountd_flags="-r"

# å…¶ä»–
  dumpdev=NO
```

**/boot/loader.conf** æ–‡ä»¶ã€‚

```ini
# cat /boot/loader.conf
# å¯åŠ¨å‚æ•°
  autoboot_delay=3
  kern.geom.label.disk_ident.enable=0
  kern.geom.label.gptid.enable=0

# ç¦ç”¨è‹±ç‰¹å°”è¶…çº¿ç¨‹
  machdep.hyperthreading_allowed=0

# åœ¨å†…æ ¸åŠ è½½å‰æ›´æ–°è‹±ç‰¹å°”å¤„ç†å™¨å¾®ç 
  cpu_microcode_load=YES
  cpu_microcode_name=/boot/firmware/intel-ucode.bin

# æ¨¡å—
  zfs_load=YES
  aio_load=YES

# RACCT/RCTL èµ„æºé™åˆ¶
  kern.racct.enable=1

# å¯åŠ¨æ—¶ç¦ç”¨å†…å­˜æµ‹è¯•
  hw.memtest.tests=0

# ç®¡é“ KVA é™åˆ¶ | 320 MB
  kern.ipc.maxpipekva=335544320

# IPC
  kern.ipc.shmseg=1024
  kern.ipc.shmmni=1024
  kern.ipc.shmseg=1024
  kern.ipc.semmns=512
  kern.ipc.semmnu=256
  kern.ipc.semume=256
  kern.ipc.semopm=256
  kern.ipc.semmsl=512

# å¤§é¡µæ˜ å°„
  vm.pmap.pg_ps_enabled=1

# ZFS è°ƒä¼˜
  vfs.zfs.prefetch_disable=1
  vfs.zfs.cache_flush_disable=1
  vfs.zfs.vdev.cache.size=16M
  vfs.zfs.arc_min=64G
  vfs.zfs.arc_max=96G

# ZFS ç¦ç”¨å¯¹è¿‡æœŸ I/O çš„ panic
  vfs.zfs.deadman_enabled=0

# NEWCONS æŒ‚èµ·
  kern.vt.suspendswitch=0
```


**/etc/sysctl.conf** æ–‡ä»¶

```ini
# cat /etc/sysctl.conf
# ZFS å¯¹é½å¤§å°
  vfs.zfs.min_auto_ashift=12

# å®‰å…¨
  security.bsd.stack_guard_page=1

# å®‰å…¨æ€§ï¼šIntel MDSï¼ˆå¾®æ¶æ„æ•°æ®é‡‡æ ·ï¼‰ç¼“è§£æªæ–½
  hw.mds_disable=3

# ç¦ç”¨æ¼äººçš„åŠŸèƒ½
  kern.coredump=0
  hw.syscons.bell=0

# IPC
  kern.ipc.shmmax=4294967296
  kern.ipc.shmall=2097152
  kern.ipc.somaxconn=4096
  kern.ipc.maxsockbuf=5242880
  kern.ipc.shm_allow_removed=1

# ç½‘ç»œ
  kern.ipc.maxsockbuf=16777216
  kern.ipc.soacceptqueue=1024
  net.inet.tcp.recvbuf_max=8388608
  net.inet.tcp.sendbuf_max=8388608
  net.inet.tcp.mssdflt=1460
  net.inet.tcp.minmss=1300
  net.inet.tcp.syncache.rexmtlimit=0
  net.inet.tcp.syncookies=0
  net.inet.tcp.tso=0
  net.inet.ip.process_options=0
  net.inet.ip.random_id=1
  net.inet.ip.redirect=0
  net.inet.icmp.drop_redirect=1
  net.inet.tcp.always_keepalive=0
  net.inet.tcp.drop_synfin=1
  net.inet.tcp.fast_finwait2_recycle=1
  net.inet.tcp.icmp_may_rst=0
  net.inet.tcp.msl=8192
  net.inet.tcp.path_mtu_discovery=0
  net.inet.udp.blackhole=1
  net.inet.tcp.blackhole=2
  net.inet.tcp.hostcache.expire=7200
  net.inet.tcp.delacktime=20
```


## ç›®çš„

ä¸ºä»€ä¹ˆè¦æ„å»ºè¿™ç§è®¾å¤‡ï¼Ÿå› ä¸ºå®ƒæ¯”è´­ä¹°â€œå“ç‰Œâ€è®¾å¤‡ä¾¿å®œå¾—å¤šã€‚ä»¥ *Dell EMC Data Domain* ä¸ºä¾‹â€”â€”è€Œä¸”ä¸æ˜¯â€œæ™®é€šâ€çš„ *Data Domain*ï¼Œè€Œæ˜¯å‡ ä¹æœ€é«˜ç«¯çš„å‹å·â€”â€”è‡³å°‘æ˜¯ *Data Domain DD9300*ã€‚å®ƒçš„ä»·æ ¼è‡³å°‘è¦é«˜åå€â€¦â€¦å®¹é‡æ›´å°ï¼Œè€Œä¸”å ç”¨æœºæ¶ç©ºé—´ä¸æ˜¯ 4Uï¼Œè€Œæ˜¯æ¥è¿‘ 14Uï¼Œéœ€è¦ä¸‰ä¸ª *DS60* æ‰©å±•å™¨ã€‚

ä½†ä½ å®é™…ä¸Šå¯ä»¥è®©è¿™ä¸ª *FreeBSD ä¼ä¸šå­˜å‚¨* çš„è¡Œä¸ºç±»ä¼¼äº *Dell EMC Data Domain*â€¦â€¦æˆ–è€…æ¯”å¦‚ä»–ä»¬çš„ *Dell EMC Elastic Cloud Storage*ã€‚

*Dell EMC CloudBoost* å¯ä»¥éƒ¨ç½²åœ¨ä½ çš„ *VMware* ç¯å¢ƒä¸­ï¼Œä»¥æä¾› DDBoost å»é‡åŠŸèƒ½ã€‚ç„¶åä½ è¿˜éœ€è¦ *OpenStack Swift*ï¼Œå› ä¸ºå®ƒæ˜¯æ”¯æŒçš„åç«¯è®¾å¤‡ä¹‹ä¸€ã€‚

![emc-cloudboost-swift-cover.png](https://vermaden.wordpress.com/wp-content/uploads/2019/06/emc-cloudboost-swift-cover.png?w=960)

![emc-cloudboost-swift-support.png](https://vermaden.wordpress.com/wp-content/uploads/2019/06/emc-cloudboost-swift-support.png?w=960)

FreeBSD ä¸Šçš„ OpenStack Swift åŒ…å¤§çº¦è½åç°å® 4-5 å¹´ï¼ˆç‰ˆæœ¬ 2.2.2ï¼‰ï¼ˆ**è¯‘æ³¨ï¼šç›®å‰ç‰ˆæœ¬å·²ç»æ›´æ–°äº†**ï¼‰ï¼Œæ‰€ä»¥è¿™é‡Œä½ éœ€è¦ä½¿ç”¨ Bhyveã€‚

```sh
# pkg search swift
(...)
py27-swift-2.2.2_1             Highly available, distributed, eventually consistent object/blob store
(...)
```

åœ¨è¿™å° *FreeBSD ä¼ä¸šå­˜å‚¨* ä¸Šå¯ä»¥åˆ›å»º Bhyve è™šæ‹Ÿæœºï¼Œä¾‹å¦‚å®‰è£… CentOS 7.6 ç³»ç»Ÿï¼Œç„¶ååœ¨å…¶ä¸­è®¾ç½® *Swift*ï¼Œè¿™æ ·å®Œå…¨å¯è¡Œã€‚åˆ©ç”¨ 20 ä¸ªç‰©ç†æ ¸å¿ƒå’Œ 128 GB å†…å­˜ï¼Œä½ å‡ ä¹æ„Ÿè§‰ä¸åˆ°è™šæ‹Ÿæœºçš„å­˜åœ¨ã€‚

è¿™æ ·ï¼Œä½ å¯ä»¥ä½¿ç”¨ *Dell EMC Networker*ï¼Œè€Œå­˜å‚¨æˆæœ¬å´é™ä½åˆ°äº†åŸæ¥çš„ååˆ†ä¹‹ä¸€è¿˜å¤šã€‚

ä»¥å‰æˆ‘ä¹Ÿå†™è¿‡å…³äº [IBM Spectrum Protect (TSM)](https://vermaden.wordpress.com/2018/08/23/ibm-tsm-spectrum-protect-on-veritas-cluster-server/) çš„æ–‡ç« ï¼Œè¿™ç§ FreeBSD ä¼ä¸šå­˜å‚¨å¯¹å®ƒä¹Ÿéå¸¸æœ‰åˆ©ã€‚æˆ‘å®é™…ä¸Šä¹Ÿå°†è¿™ä¸ªåŸºäº FreeBSD çš„å­˜å‚¨ç”¨ä½œ *IBM Spectrum Protect (TSM)* å®¹å™¨æ± ç›®å½•çš„ç©ºé—´ã€‚é€šè¿‡ iSCSI å¯¼å‡ºæ•ˆæœéå¸¸å¥½ã€‚

ä½ è¿˜å¯ä»¥å°†è¿™ä¸ª *FreeBSD ä¼ä¸šå­˜å‚¨* ä¸å…¶ä»–å­˜å‚¨è®¾å¤‡è¿›è¡Œæ¯”è¾ƒï¼Œæ¯”å¦‚ *iXsystems TrueNAS* æˆ– *EXAGRID*ã€‚

## æ€§èƒ½

ä½ è‚¯å®šæƒ³çŸ¥é“è¿™ä¸ª *FreeBSD ä¼ä¸šå­˜å‚¨* çš„æ€§èƒ½æ°´å¹³ :ğŸ™‚:

æˆ‘ä¼šåˆ†äº«æˆ‘æ”¶é›†åˆ°çš„æ‰€æœ‰æ€§èƒ½æ•°æ®ã€‚

### ç½‘ç»œæ€§èƒ½

é¦–å…ˆæ˜¯ç½‘ç»œæ€§èƒ½ã€‚

æˆ‘ä½¿ç”¨ **iperf3** ä½œä¸ºåŸºå‡†æµ‹è¯•å·¥å…·ã€‚

æˆ‘åœ¨ FreeBSD ç«¯å¯åŠ¨æœåŠ¡å™¨ï¼š

```sh
# iperf3 -s
```

ç„¶ååœ¨ *Windows Server 2016* æœºå™¨ä¸Šå¯åŠ¨å®¢æˆ·ç«¯ï¼š

```sh
C:\iperf-3.1.3-win64>iperf3.exe -c nas02 -P 8
(...)
[SUM]   0.00-10.00  sec  10.8 GBytes  9.26 Gbits/sec                  receiver
(..)
```

è¿™æ˜¯åœ¨ MTU 1500 ä¸‹çš„ç»“æœâ€”â€”ä¸ä½¿ç”¨ Jumbo å¸§ :ğŸ˜¦:

ä¸å¹¸çš„æ˜¯ï¼Œè¿™å¥—ç³»ç»Ÿåªæœ‰ä¸€ä¸ªç‰©ç† 10GE æ¥å£ï¼Œä½†æˆ‘ä¹Ÿåšäº†å…¶ä»–æµ‹è¯•ã€‚ä½¿ç”¨ä¸¤å°å• 10GE æ¥å£çš„æœåŠ¡å™¨ï¼Œå¯ä»¥å¾ˆå¥½åœ°é¥±å’Œ FreeBSD ç«¯çš„åŒ 10GE LACPã€‚

æˆ‘è¿˜å°† NFS å’Œ iSCSI å¯¼å‡ºåˆ° *RHEL* ç³»ç»Ÿã€‚å•ä¸ª 10GE æ¥å£ä¸‹ç½‘ç»œæ€§èƒ½çº¦ä¸º 500-600 MB/sã€‚åœ¨ LACP èšåˆä¸‹å¯ä»¥è¾¾åˆ° 1000-1200 MB/sã€‚

### ç£ç›˜å­ç³»ç»Ÿæ€§èƒ½

ç°åœ¨æ˜¯ç£ç›˜å­ç³»ç»Ÿã€‚

å…ˆç”¨ä¸€äº›ç®€å•çš„æµ‹è¯•ï¼Œä½¿ç”¨ FreeBSD è‡ªå¸¦å·¥å…· **diskinfo(8)**ã€‚

```sh
# diskinfo -ctv /dev/da12
/dev/da12
        512             # sectorsize
        12000138625024  # mediasize in bytes (11T)
        23437770752     # mediasize in sectors
        4096            # stripesize
        0               # stripeoffset
        1458933         # Cylinders according to firmware.
        255             # Heads according to firmware.
        63              # Sectors according to firmware.
        ATA TOSHIBA MG07ACA1    # Disk descr.
        98H0A11KF95G    # Disk ident.
        id1,enc@n500e081010445dbd/type@0/slot@c/elmdesc@ArrayDevice11   # Physical path
        No              # TRIM/UNMAP support
        7200            # Rotation rate in RPM
        Not_Zoned       # Zone Mode

I/O command overhead:
        time to read 10MB block      0.067031 sec       =    0.003 msec/sector
        time to read 20480 sectors   2.619989 sec       =    0.128 msec/sector
        calculated command overhead                     =    0.125 msec/sector

Seek times:
        Full stroke:      250 iter in   5.665880 sec =   22.664 msec
        Half stroke:      250 iter in   4.263047 sec =   17.052 msec
        Quarter stroke:   500 iter in   6.867914 sec =   13.736 msec
        Short forward:    400 iter in   3.057913 sec =    7.645 msec
        Short backward:   400 iter in   1.979287 sec =    4.948 msec
        Seq outer:       2048 iter in   0.169472 sec =    0.083 msec
        Seq inner:       2048 iter in   0.469630 sec =    0.229 msec

Transfer rates:
        outside:       102400 kbytes in   0.478251 sec =   214114 kbytes/sec
        middle:        102400 kbytes in   0.605701 sec =   169060 kbytes/sec
        inside:        102400 kbytes in   1.303909 sec =    78533 kbytes/sec
```

ç°åœ¨æˆ‘ä»¬å·²ç»çŸ¥é“å•å—ç£ç›˜çš„é€Ÿåº¦äº†ã€‚

æ¥ä¸‹æ¥ï¼Œè®©æˆ‘ä»¬åœ¨ ZFS zvol è®¾å¤‡ä¸Šé‡å¤åŒæ ·çš„æµ‹è¯•ã€‚

```sh
# diskinfo -ctv /dev/zvol/nas02/iscsi/test
/dev/zvol/nas02/iscsi/test
        512             # sectorsize
        17592186044416  # mediasize in bytes (16T)
        34359738368     # mediasize in sectors
        65536           # stripesize
        0               # stripeoffset
        Yes             # TRIM/UNMAP support
        Unknown         # Rotation rate in RPM

I/O command overhead:
        time to read 10MB block      0.004512 sec       =    0.000 msec/sector
        time to read 20480 sectors   0.196824 sec       =    0.010 msec/sector
        calculated command overhead                     =    0.009 msec/sector

Seek times:
        Full stroke:      250 iter in   0.006151 sec =    0.025 msec
        Half stroke:      250 iter in   0.008228 sec =    0.033 msec
        Quarter stroke:   500 iter in   0.014062 sec =    0.028 msec
        Short forward:    400 iter in   0.010564 sec =    0.026 msec
        Short backward:   400 iter in   0.011725 sec =    0.029 msec
        Seq outer:       2048 iter in   0.028198 sec =    0.014 msec
        Seq inner:       2048 iter in   0.028416 sec =    0.014 msec

Transfer rates:
        outside:       102400 kbytes in   0.036938 sec =  2772213 kbytes/sec
        middle:        102400 kbytes in   0.043076 sec =  2377194 kbytes/sec
        inside:        102400 kbytes in   0.034260 sec =  2988908 kbytes/sec
```

æ¥è¿‘ 3 GB/s â€”â€” ä¸é”™ã€‚

æ¥ä¸‹æ¥è¿›è¡Œæ›´ä¼ ç»Ÿçš„æµ‹è¯• â€”â€” ä¸æœ½çš„ **dd(8)** å‘½ä»¤ã€‚

è¿™æ˜¯åœ¨ **compression=off** è®¾ç½®ä¸‹è¿›è¡Œçš„æµ‹è¯•ã€‚

å•è¿›ç¨‹è¿è¡Œã€‚

```sh
# dd if=/dev/zero of=FILE bs=128m status=progress
26172456960 bytes (26 GB, 24 GiB) transferred 16.074s, 1628 MB/s
202+0 records in
201+0 records out
26977763328 bytes transferred in 16.660884 secs (1619227644 bytes/sec)
```

å››ä¸ªå¹¶å‘è¿›ç¨‹è¿è¡Œã€‚

```sh
# dd if=/dev/zero of=FILE${X} bs=128m status=progress
80933289984 bytes (81 GB, 75 GiB) transferred 98.081s, 825 MB/s
608+0 records in
608+0 records out
81604378624 bytes transferred in 98.990579 secs (824365101 bytes/sec)
```

å…«ä¸ªå¹¶å‘è¿›ç¨‹è¿è¡Œã€‚

```sh
# dd if=/dev/zero of=FILE${X} bs=128m status=progress
174214610944 bytes (174 GB, 162 GiB) transferred 385.042s, 452 MB/s
1302+0 records in
1301+0 records out
174617264128 bytes transferred in 385.379296 secs (453104943 bytes/sec)
```

æ€»ç»“è¿™äº›æ•°æ®ã€‚

```sh
1 STREAM(s) ~ 1600 MB/s ~ 1.5 GB/s
4 STREAM(s) ~ 3300 MB/s ~ 3.2 GB/s
8 STREAM(s) ~ 3600 MB/s ~ 3.5 GB/s
```

æ‰€ä»¥ç£ç›˜å­ç³»ç»Ÿåœ¨é¡ºåºå†™å…¥æ—¶èƒ½å¤Ÿè¾¾åˆ°çº¦ 3.5 GB/s çš„æŒç»­é€Ÿåº¦ã€‚è¿™æ„å‘³ç€å¦‚æœæƒ³è¦å®Œå…¨é¥±å’Œç½‘ç»œï¼Œéœ€è¦å†æ·»åŠ ä¸¤ä¸ª 10GE æ¥å£ã€‚

ç£ç›˜çš„å‹åŠ›æµ‹è¯•ä»…è¾¾åˆ°çº¦ 55%ï¼Œè¿™å¯ä»¥é€šè¿‡ FreeBSD çš„å¦ä¸€ä¸ªå®ç”¨å·¥å…· **gstat(8)** å‘½ä»¤çœ‹åˆ°ã€‚

![n10.png](https://vermaden.wordpress.com/wp-content/uploads/2019/06/n10.png?w=960)

æ¥ä¸‹æ¥è¿›è¡Œæ›´â€œæ™ºèƒ½â€çš„æµ‹è¯•â€”â€”**blogbench** æµ‹è¯•ã€‚

é¦–å…ˆç¦ç”¨å‹ç¼©è¿›è¡Œæµ‹è¯•ã€‚

```sh
# time blogbench -d .
Frequency = 10 secs
Scratch dir = [.]
Spawning 3 writers...
Spawning 1 rewriters...
Spawning 5 commenters...
Spawning 100 readers...
Benchmarking for 30 iterations.
The test will run during 5 minutes.
(...)
Final score for writes:          6476
Final score for reads :        660436

blogbench -d .  280.58s user 4974.41s system 1748% cpu 5:00.54 total
```

æ¥ä¸‹æ¥è®¾ç½®å‹ç¼©ä¸º LZ4 è¿›è¡Œç¬¬äºŒè½®æµ‹è¯•ã€‚

```sh
# time blogbench -d .
Frequency = 10 secs
Scratch dir = [.]
Spawning 3 writers...
Spawning 1 rewriters...
Spawning 5 commenters...
Spawning 100 readers...
Benchmarking for 30 iterations.
The test will run during 5 minutes.
(...)
Final score for writes:          7087
Final score for reads :        733932

blogbench -d .  299.08s user 5415.04s system 1900% cpu 5:00.68 total
```

å‹ç¼©æ•ˆæœä¸å¤§ï¼Œä½†ç¡®å®æœ‰ä¸€å®šå¸®åŠ©ã€‚

ä¸ºäº†å¯¹æ¯”ï¼Œæˆ‘ä»¬å°†åœ¨ç³»ç»Ÿ ZFS æ± ä¸Šè¿è¡Œç›¸åŒçš„æµ‹è¯•â€”â€”ä¸¤ä¸ª Intel SSD DC S3500 240 GB é•œåƒé©±åŠ¨ï¼Œå…¶æ€§èƒ½å¦‚ä¸‹ï¼š

**Intel SSD DC S3500 240 GB é©±åŠ¨ç‰¹æ€§ï¼š**

* é¡ºåºè¯»å–ï¼ˆæœ€å¤§ï¼‰**500 MB/s**
* é¡ºåºå†™å…¥ï¼ˆæœ€å¤§ï¼‰**260 MB/s**
* éšæœºè¯»å–ï¼ˆ100% èŒƒå›´ï¼‰**75000 IOPS**
* éšæœºå†™å…¥ï¼ˆ100% èŒƒå›´ï¼‰**7500 IOPS**

```sh
# time blogbench -d .
Frequency = 10 secs
Scratch dir = [.]
Spawning 3 writers...
Spawning 1 rewriters...
Spawning 5 commenters...
Spawning 100 readers...
Benchmarking for 30 iterations.
The test will run during 5 minutes.
(...)
Final score for writes:          6109
Final score for reads :        654099

blogbench -d .  278.73s user 5058.75s system 1777% cpu 5:00.30 total
```

ç°åœ¨è¿›è¡Œ **randomio** æµ‹è¯•ã€‚è¿™æ˜¯ä¸€ä¸ªå¤šçº¿ç¨‹ç£ç›˜ I/O å¾®åŸºå‡†æµ‹è¯•ã€‚

ä½¿ç”¨æ–¹æ³•å¦‚ä¸‹ï¼š

```sh
usage: randomio filename nr_threads write_fraction_of_io fsync_fraction_of_writes io_size nr_seconds_between_samples

filename                    Filename or device to read/write.
write_fraction_of_io        What fraction of I/O should be writes - for example 0.25 for 25% write.
fsync_fraction_of_writes    What fraction of writes should be fsync'd.
io_size                     How many bytes to read/write (multiple of 512 bytes).
nr_seconds_between_samples  How many seconds to average samples over.
```

ä½¿ç”¨ **4k å—** è¿›è¡Œçš„ **éšæœº I/O** æµ‹è¯•ã€‚

```sh
# zfs create -s -V 1T nas02/iscsi/test
# randomio /dev/zvol/nas02/iscsi/test 8 0.25 1 4096 10
  total |  read:         latency (ms)       |  write:        latency (ms)
   iops |   iops   min    avg    max   sdev |   iops   min    avg    max   sdev
--------+-----------------------------------+----------------------------------
54137.7 |40648.4   0.0    0.1  575.8    2.2 |13489.4   0.0    0.3  405.8    2.6
66248.4 |49641.5   0.0    0.1   19.6    0.3 |16606.9   0.0    0.2   26.4    0.7
66411.0 |49817.2   0.0    0.1   19.7    0.3 |16593.8   0.0    0.2   20.3    0.7
64158.9 |48142.8   0.0    0.1  254.7    0.7 |16016.1   0.0    0.2  130.4    1.0
48454.1 |36390.8   0.0    0.1  542.8    2.7 |12063.3   0.0    0.3  507.5    3.2
66796.1 |50067.4   0.0    0.1   24.1    0.3 |16728.7   0.0    0.2   23.4    0.7
58512.2 |43851.7   0.0    0.1  576.5    1.7 |14660.5   0.0    0.2  307.2    1.7
63195.8 |47341.8   0.0    0.1  261.6    0.9 |15854.1   0.0    0.2  361.1    1.9
67086.0 |50335.6   0.0    0.1   20.4    0.3 |16750.4   0.0    0.2   25.1    0.8
67429.8 |50549.6   0.0    0.1   21.8    0.3 |16880.3   0.0    0.2   20.6    0.7
^C
```

â€¦ ä½¿ç”¨ **512 å­—èŠ‚æ‰‡åŒº** è¿›è¡Œæµ‹è¯•ã€‚


```sh
# zfs create -s -V 1T nas02/iscsi/test
# randomio /dev/zvol/nas02/iscsi/TEST 8 0.25 1 512 10
  total |  read:         latency (ms)       |  write:        latency (ms)
   iops |   iops   min    avg    max   sdev |   iops   min    avg    max   sdev
--------+-----------------------------------+----------------------------------
58218.9 |43712.0   0.0    0.1  501.5    2.1 |14506.9   0.0    0.2  272.5    1.6
66325.3 |49703.8   0.0    0.1  352.0    0.9 |16621.4   0.0    0.2  352.0    1.5
68130.5 |51100.8   0.0    0.1   24.6    0.3 |17029.7   0.0    0.2   24.4    0.7
68465.3 |51352.3   0.0    0.1   19.9    0.3 |17112.9   0.0    0.2   23.8    0.7
54903.5 |41249.1   0.0    0.1  399.3    1.9 |13654.4   0.0    0.3  335.8    2.2
61259.8 |45898.7   0.0    0.1  574.6    1.7 |15361.0   0.0    0.2  371.5    1.7
68483.3 |51313.1   0.0    0.1   22.9    0.3 |17170.3   0.0    0.2   26.1    0.7
56713.7 |42524.7   0.0    0.1  373.5    1.8 |14189.1   0.0    0.2  438.5    2.7
68861.4 |51657.0   0.0    0.1   21.0    0.3 |17204.3   0.0    0.2   21.7    0.7
68602.0 |51438.4   0.0    0.1   19.5    0.3 |17163.7   0.0    0.2   23.7    0.7
^C
```

ä¸¤ä¸ª **éšæœº I/O** æµ‹è¯•éƒ½æ˜¯åœ¨å¯ç”¨ LZ4 å‹ç¼©çš„æƒ…å†µä¸‹è¿è¡Œçš„ã€‚

æ¥ä¸‹æ¥æ˜¯ **bonnie++** åŸºå‡†æµ‹è¯•ã€‚åŒæ ·æ˜¯åœ¨å¯ç”¨ LZ4 å‹ç¼©çš„æƒ…å†µä¸‹è¿è¡Œçš„ã€‚


```sh
# bonnie++ -d . -u root
Using uid:0, gid:0.
Writing a byte at a time...done
Writing intelligently...done
Rewriting...done
Reading a byte at a time...done
Reading intelligently...done
start 'em...done...done...done...done...done...
Create files in sequential order...done.
Stat files in sequential order...done.
Delete files in sequential order...done.
Create files in random order...done.
Stat files in random order...done.
Delete files in random order...done.
Version  1.97       ------Sequential Output------ --Sequential Input- --Random-
Concurrency   1     -Per Chr- --Block-- -Rewrite- -Per Chr- --Block-- --Seeks--
Machine        Size K/sec %CP K/sec %CP K/sec %CP K/sec %CP K/sec %CP  /sec %CP
nas02.local 261368M   139  99 775132  99 589190  99   383  99 1638929  99 12930 2046
Latency             60266us    7030us    7059us   21553us    3844us    5710us
Version  1.97       ------Sequential Create------ --------Random Create--------
nas02.local         -Create-- --Read--- -Delete-- -Create-- --Read--- -Delete--
              files  /sec %CP  /sec %CP  /sec %CP  /sec %CP  /sec %CP  /sec %CP
                 16 +++++ +++ +++++ +++ 12680  44 +++++ +++ +++++ +++ 30049  99
Latency              2619us      43us     714ms    2748us      28us      58us
```

æœ€åä½†åŒæ ·é‡è¦çš„æ˜¯ **fio** åŸºå‡†æµ‹è¯•ã€‚åŒæ ·å¯ç”¨äº† LZ4 å‹ç¼©ã€‚

```sh
# fio --randrepeat=1 --direct=1 --gtod_reduce=1 --name=test --filename=random_read_write.fio --bs=4k --iodepth=64 --size=4G --readwrite=randrw --rwmixread=75
test: (g=0): rw=randrw, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=psync, iodepth=64
fio-3.13
Starting 1 process
Jobs: 1 (f=1): [m(1)][98.0%][r=38.0MiB/s,w=12.2MiB/s][r=9735,w=3128 IOPS][eta 00m:05s]
test: (groupid=0, jobs=1): err= 0: pid=35368: Tue Jun 18 15:14:44 2019
  read: IOPS=3157, BW=12.3MiB/s (12.9MB/s)(3070MiB/248872msec)
   bw (  KiB/s): min= 9404, max=57732, per=98.72%, avg=12469.84, stdev=3082.99, samples=497
   iops        : min= 2351, max=14433, avg=3117.15, stdev=770.74, samples=497
  write: IOPS=1055, BW=4222KiB/s (4323kB/s)(1026MiB/248872msec)
   bw (  KiB/s): min= 3179, max=18914, per=98.71%, avg=4166.60, stdev=999.23, samples=497
   iops        : min=  794, max= 4728, avg=1041.25, stdev=249.76, samples=497
  cpu          : usr=1.11%, sys=88.64%, ctx=677981, majf=0, minf=0
  IO depths    : 1=100.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, >=64=0.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     issued rwts: total=785920,262656,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=64

Run status group 0 (all jobs):
   READ: bw=12.3MiB/s (12.9MB/s), 12.3MiB/s-12.3MiB/s (12.9MB/s-12.9MB/s), io=3070MiB (3219MB), run=248872-248872msec
  WRITE: bw=4222KiB/s (4323kB/s), 4222KiB/s-4222KiB/s (4323kB/s-4323kB/s), io=1026MiB (1076MB), run=248872-248872msec
```

ä¸çŸ¥é“ä½ æ€ä¹ˆçœ‹ï¼Œæˆ‘å¯¹æ€§èƒ½æ˜¯éå¸¸æ»¡æ„çš„ :ğŸ™‚:

## FreeNAS

æœ€åˆæˆ‘ç¡®å®æƒ³åœ¨è¿™äº›æœåŠ¡å™¨ä¸Šä½¿ç”¨ *FreeNAS*ï¼Œç”šè‡³è¿˜å®‰è£…äº† *FreeNAS*ã€‚å®ƒè¿è¡Œå¾—ä¸é”™ï¼Œä½†â€¦â€¦ *FreeNAS* çš„å®‰å…¨éƒ¨åˆ†å¹¶ä¸æ˜¯æœ€ä½³ã€‚

è¿™æ˜¯ **pkg audit** å‘½ä»¤çš„è¾“å‡ºï¼Œéå¸¸å“äººã€‚

```sh
root@freenas[~]# pkg audit -F
Fetching vuln.xml.bz2: 100%  785 KiB 804.3kB/s    00:01
python27-2.7.15 is vulnerable:
Python -- NULL pointer dereference vulnerability
CVE: CVE-2019-5010
WWW: https://vuxml.FreeBSD.org/freebsd/d74371d2-4fee-11e9-a5cd-1df8a848de3d.html

curl-7.62.0 is vulnerable:
curl -- multiple vulnerabilities
CVE: CVE-2019-3823
CVE: CVE-2019-3822
CVE: CVE-2018-16890
WWW: https://vuxml.FreeBSD.org/freebsd/714b033a-2b09-11e9-8bc3-610fd6e6cd05.html

libgcrypt-1.8.2 is vulnerable:
libgcrypt -- side-channel attack vulnerability
CVE: CVE-2018-0495
WWW: https://vuxml.FreeBSD.org/freebsd/9b5162de-6f39-11e8-818e-e8e0b747a45a.html

python36-3.6.5_1 is vulnerable:
Python -- NULL pointer dereference vulnerability
CVE: CVE-2019-5010
WWW: https://vuxml.FreeBSD.org/freebsd/d74371d2-4fee-11e9-a5cd-1df8a848de3d.html

pango-1.42.0 is vulnerable:
pango -- remote DoS vulnerability
CVE: CVE-2018-15120
WWW: https://vuxml.FreeBSD.org/freebsd/5a757a31-f98e-4bd4-8a85-f1c0f3409769.html

py36-requests-2.18.4 is vulnerable:
www/py-requests -- Information disclosure vulnerability
WWW: https://vuxml.FreeBSD.org/freebsd/50ad9a9a-1e28-11e9-98d7-0050562a4d7b.html

libnghttp2-1.31.0 is vulnerable:
nghttp2 -- Denial of service due to NULL pointer dereference
CVE: CVE-2018-1000168
WWW: https://vuxml.FreeBSD.org/freebsd/1fccb25e-8451-438c-a2b9-6a021e4d7a31.html

gnupg-2.2.6 is vulnerable:
gnupg -- unsanitized output (CVE-2018-12020)
CVE: CVE-2017-7526
CVE: CVE-2018-12020
WWW: https://vuxml.FreeBSD.org/freebsd/7da0417f-6b24-11e8-84cc-002590acae31.html

py36-cryptography-2.1.4 is vulnerable:
py-cryptography -- tag forgery vulnerability
CVE: CVE-2018-10903
WWW: https://vuxml.FreeBSD.org/freebsd/9e2d0dcf-9926-11e8-a92d-0050562a4d7b.html

perl5-5.26.1 is vulnerable:
perl -- multiple vulnerabilities
CVE: CVE-2018-6913
CVE: CVE-2018-6798
CVE: CVE-2018-6797
WWW: https://vuxml.FreeBSD.org/freebsd/41c96ffd-29a6-4dcc-9a88-65f5038fa6eb.html

libssh2-1.8.0,3 is vulnerable:
libssh2 -- multiple issues
CVE: CVE-2019-3862
CVE: CVE-2019-3861
CVE: CVE-2019-3860
CVE: CVE-2019-3858
WWW: https://vuxml.FreeBSD.org/freebsd/6e58e1e9-2636-413e-9f84-4c0e21143628.html

git-lite-2.17.0 is vulnerable:
Git -- Fix memory out-of-bounds and remote code execution vulnerabilities (CVE-2018-11233 and CVE-2018-11235)
CVE: CVE-2018-11235
CVE: CVE-2018-11233
WWW: https://vuxml.FreeBSD.org/freebsd/c7a135f4-66a4-11e8-9e63-3085a9a47796.html

gnutls-3.5.18 is vulnerable:
GnuTLS -- double free, invalid pointer access
CVE: CVE-2019-3836
CVE: CVE-2019-3829
WWW: https://vuxml.FreeBSD.org/freebsd/fb30db8f-62af-11e9-b0de-001cc0382b2f.html

13 problem(s) in the installed packages found.

root@freenas[~]# uname -a
FreeBSD freenas.local 11.2-STABLE FreeBSD 11.2-STABLE #0 r325575+95cc58ca2a0(HEAD): Mon May  6 19:08:58 EDT 2019     root@mp20.tn.ixsystems.com:/freenas-releng/freenas/_BE/objs/freenas-releng/freenas/_BE/os/sys/FreeNAS.amd64  amd64

root@freenas[~]# freebsd-version -uk
11.2-STABLE
11.2-STABLE

root@freenas[~]# sockstat -l4
USER     COMMAND    PID   FD PROTO  LOCAL ADDRESS         FOREIGN ADDRESS
root     uwsgi-3.6  4006  3  tcp4   127.0.0.1:9042        *:*
root     uwsgi-3.6  3188  3  tcp4   127.0.0.1:9042        *:*
nobody   mdnsd      3144  4  udp4   *:31417               *:*
nobody   mdnsd      3144  6  udp4   *:5353                *:*
www      nginx      3132  6  tcp4   *:443                 *:*
www      nginx      3132  8  tcp4   *:80                  *:*
root     nginx      3131  6  tcp4   *:443                 *:*
root     nginx      3131  8  tcp4   *:80                  *:*
root     ntpd       2823  21 udp4   *:123                 *:*
root     ntpd       2823  22 udp4   10.49.13.99:123       *:*
root     ntpd       2823  25 udp4   127.0.0.1:123         *:*
root     sshd       2743  5  tcp4   *:22                  *:*
root     syslog-ng  2341  19 udp4   *:1031                *:*
nobody   mdnsd      2134  3  udp4   *:39020               *:*
nobody   mdnsd      2134  5  udp4   *:5353                *:*
root     python3.6  236   22 tcp4   *:6000                *:*
```

æˆ‘ç”šè‡³è¯•å›¾äº†è§£ä¸ºä»€ä¹ˆ *FreeNAS* åœ¨å…¶æœ€æ–°ç‰ˆæœ¬ä¸­ä¼šåŒ…å«å¦‚æ­¤è¿‡æ—¶ä¸”ä¸å®‰å…¨çš„åŒ…â€”â€”[FreeNAS 11.2-U3 Vulnerabilities](https://www.ixsystems.com/community/threads/freenas-11-2-u3-vulnerabilities.75353/)â€”â€”è¿™æ˜¯æˆ‘åœ¨ä»–ä»¬è®ºå›ä¸Šå‘èµ·çš„è®¨è®ºè´´ã€‚

ä¸å¹¸çš„æ˜¯ï¼Œè¿™åæ˜ äº†ä»–ä»¬çš„æ”¿ç­–ï¼Œå¯ä»¥æ€»ç»“ä¸ºâ€œåªè¦èƒ½ç”¨ï¼Œå°±ä¸è¦åŠ¨/æ›´æ”¹ç‰ˆæœ¬â€â€”â€”è‡³å°‘æˆ‘å¾—å‡ºäº†è¿™ä¸ªå°è±¡ã€‚

å› ä¸ºè¿™äº›å®‰å…¨æ¼æ´ï¼Œæˆ‘æ— æ³•æ¨èä½¿ç”¨ *FreeNAS*ï¼Œäºæ˜¯æˆ‘è½¬å‘äº†åŸç”Ÿçš„ FreeBSD ç³»ç»Ÿã€‚

å¦ä¸€ä¸ªæœ‰è¶£çš„æƒ…å†µæ˜¯ï¼Œåœ¨æˆ‘å®‰è£… FreeBSD åï¼Œæˆ‘æƒ³å¯¼å…¥ *FreeNAS* åˆ›å»ºçš„ ZFS poolã€‚è¿™æ˜¯æˆ‘æ‰§è¡Œ **zpool import** å‘½ä»¤åçš„ç»“æœã€‚

```sh
# zpool import
   pool: nas02_gr06
     id: 1275660523517109367
  state: ONLINE
 status: The pool was last accessed by another system.
 action: The pool can be imported using its name or numeric identifier and
        the '-f' flag.
   see: http://illumos.org/msg/ZFS-8000-EY
 config:

        nas02_gr06  ONLINE
          raidz2-0  ONLINE
            da58p2  ONLINE
            da59p2  ONLINE
            da60p2  ONLINE
            da61p2  ONLINE
            da62p2  ONLINE
            da63p2  ONLINE
            da64p2  ONLINE
            da26p2  ONLINE
            da65p2  ONLINE
            da23p2  ONLINE
            da29p2  ONLINE
            da66p2  ONLINE
            da67p2  ONLINE
            da68p2  ONLINE
        spares
          da69p2

   pool: nas02_gr05
     id: 5642709896812665361
  state: ONLINE
 status: The pool was last accessed by another system.
 action: The pool can be imported using its name or numeric identifier and
        the '-f' flag.
   see: http://illumos.org/msg/ZFS-8000-EY
 config:

        nas02_gr05  ONLINE
          raidz2-0  ONLINE
            da20p2  ONLINE
            da30p2  ONLINE
            da34p2  ONLINE
            da50p2  ONLINE
            da28p2  ONLINE
            da38p2  ONLINE
            da51p2  ONLINE
            da52p2  ONLINE
            da27p2  ONLINE
            da32p2  ONLINE
            da53p2  ONLINE
            da54p2  ONLINE
            da55p2  ONLINE
            da56p2  ONLINE
        spares
          da57p2

   pool: nas02_gr04
     id: 2460983830075205166
  state: ONLINE
 status: The pool was last accessed by another system.
 action: The pool can be imported using its name or numeric identifier and
        the '-f' flag.
   see: http://illumos.org/msg/ZFS-8000-EY
 config:

        nas02_gr04  ONLINE
          raidz2-0  ONLINE
            da44p2  ONLINE
            da37p2  ONLINE
            da18p2  ONLINE
            da36p2  ONLINE
            da45p2  ONLINE
            da19p2  ONLINE
            da22p2  ONLINE
            da33p2  ONLINE
            da35p2  ONLINE
            da21p2  ONLINE
            da31p2  ONLINE
            da47p2  ONLINE
            da48p2  ONLINE
            da49p2  ONLINE
        spares
          da46p2

   pool: nas02_gr03
     id: 4878868173820164207
  state: ONLINE
 status: The pool was last accessed by another system.
 action: The pool can be imported using its name or numeric identifier and
        the '-f' flag.
   see: http://illumos.org/msg/ZFS-8000-EY
 config:

        nas02_gr03  ONLINE
          raidz2-0  ONLINE
            da81p2  ONLINE
            da71p2  ONLINE
            da14p2  ONLINE
            da15p2  ONLINE
            da80p2  ONLINE
            da16p2  ONLINE
            da88p2  ONLINE
            da17p2  ONLINE
            da40p2  ONLINE
            da41p2  ONLINE
            da25p2  ONLINE
            da42p2  ONLINE
            da24p2  ONLINE
            da43p2  ONLINE
        spares
          da39p2

   pool: nas02_gr02
     id: 3299037437134217744
  state: ONLINE
 status: The pool was last accessed by another system.
 action: The pool can be imported using its name or numeric identifier and
        the '-f' flag.
   see: http://illumos.org/msg/ZFS-8000-EY
 config:

        nas02_gr02  ONLINE
          raidz2-0  ONLINE
            da84p2  ONLINE
            da76p2  ONLINE
            da85p2  ONLINE
            da8p2   ONLINE
            da9p2   ONLINE
            da78p2  ONLINE
            da73p2  ONLINE
            da74p2  ONLINE
            da70p2  ONLINE
            da77p2  ONLINE
            da11p2  ONLINE
            da13p2  ONLINE
            da79p2  ONLINE
            da89p2  ONLINE
        spares
          da90p2

   pool: nas02_gr01
     id: 1132383125952900182
  state: ONLINE
 status: The pool was last accessed by another system.
 action: The pool can be imported using its name or numeric identifier and
        the '-f' flag.
   see: http://illumos.org/msg/ZFS-8000-EY
 config:

        nas02_gr01  ONLINE
          raidz2-0  ONLINE
            da91p2  ONLINE
            da75p2  ONLINE
            da0p2   ONLINE
            da82p2  ONLINE
            da1p2   ONLINE
            da83p2  ONLINE
            da2p2   ONLINE
            da3p2   ONLINE
            da4p2   ONLINE
            da5p2   ONLINE
            da86p2  ONLINE
            da6p2   ONLINE
            da7p2   ONLINE
            da72p2  ONLINE
        spares
          da87p2
```

çœ‹èµ·æ¥ *FreeNAS* å¯¹ ZFS çš„å¤„ç†æ–¹å¼ç•¥æœ‰ä¸åŒï¼Œä»–ä»¬ä¸ºæ¯ä¸ª RAIDZ2 ç›®æ ‡åˆ›å»ºäº†å•ç‹¬çš„ poolï¼Œå¹¶é…ç½®äº†ä¸“ç”¨çš„å¤‡ç”¨ç›˜ã€‚æœ‰è¶£â€¦â€¦

## æ›´æ–° 1 â€“ BSD Now 305

[FreeBSD Enterprise 1 PB Storage](https://vermaden.wordpress.com/2019/06/19/freebsd-enterprise-1-pb-storage/) æ–‡ç« å‡ºç°åœ¨ [BSD Now 305 â€“ Changing Face of Unix](https://www.bsdnow.tv/305) èŠ‚ç›®ä¸­ã€‚

æ„Ÿè°¢æåŠï¼

## æ›´æ–° 2 â€“ æ•°æ®ä¸­å¿ƒå®æ™¯ç…§ç‰‡

æœ‰äº›è¯»è€…å¸Œæœ›çœ‹åˆ°è¿™å°â€œæ€ªå…½â€çš„å®æ™¯ç…§ç‰‡ã€‚ä¸‹é¢æ˜¯æ•°æ®ä¸­å¿ƒæ‹æ‘„çš„å‡ å¼ å›¾ç‰‡ã€‚

æœºç®±æ­£é¢åŠå¸ƒçº¿ã€‚

![tyan-real-01.jpg](https://vermaden.wordpress.com/wp-content/uploads/2019/06/tyan-real-01.jpg?w=960)

æœºç®±æ­£é¢å¦ä¸€è§’åº¦ã€‚

![tyan-real-09.jpg](https://vermaden.wordpress.com/wp-content/uploads/2019/06/tyan-real-09.jpg?w=960)

æœºç®±èƒŒé¢åŠå¸ƒçº¿ã€‚

![tyan-real-02.jpg](https://vermaden.wordpress.com/wp-content/uploads/2019/06/tyan-real-02.jpg?w=960)

å¸¦ç¡¬ç›˜çš„é¡¶éƒ¨è§†è§’ã€‚

![tyan-real-03](https://vermaden.wordpress.com/wp-content/uploads/2019/06/tyan-real-03.jpg?w=960)

é¡¶éƒ¨å¦ä¸€è§†è§’ã€‚

![tyan-real-07.jpg](https://vermaden.wordpress.com/wp-content/uploads/2019/06/tyan-real-07.jpg?w=960)

ç¡¬ç›˜ä½ç‰¹å†™ã€‚

![tyan-real-08.jpg](https://vermaden.wordpress.com/wp-content/uploads/2019/06/tyan-real-08.jpg?w=960)

å›ºæ€ç¡¬ç›˜ä¸æœºæ¢°ç¡¬ç›˜ã€‚

![tyan-real-06.jpg](https://vermaden.wordpress.com/wp-content/uploads/2019/06/tyan-real-06.jpg?w=960)
